# -*- coding: utf-8 -*-
"""immmages.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13f2pcKLDDpcRHe0VJVQzF43me2Qohuyo

# ***Test Code***
"""

# import tkinter as tk
# from tkinter import filedialog

# def choose_file():
#     file_path = filedialog.askopenfilename(title="Select an image file", filetypes=[("Image files", "*.png;*.jpg;*.jpeg")])
#     if file_path:
#         label.config(text=f"Selected File: {file_path}")
#         # Perform further actions with the selected file path, e.g., read and process the image

# # Create the main window
# root = tk.Tk()
# root.title("File Chooser")

# # Create a label to display the selected file path
# label = tk.Label(root, text="Selected File: ")
# label.pack()

# # Create a "Choose File" button
# button = tk.Button(root, text="Choose File", command=choose_file)
# button.pack()

# # Start the Tkinter event loop
# root.mainloop()

# from google.colab import files
# import cv2
# import numpy as np

# def upload_image():
#     uploaded = files.upload()
#     if not uploaded:
#         print("No file uploaded. Exiting.")
#         return None
#     file_name = next(iter(uploaded))
#     return cv2.imdecode(np.frombuffer(uploaded[file_name], np.uint8), -1)

# def modify_image(image, test_input):
#     # Execute modifications based on test_input
#     modified_image = apply_modifications(image, test_input)

#     # Save the modified image
#     cv2.imwrite('modified_image.jpg', modified_image)

# def apply_modifications(image, test_input):
#     # Your modification code here
#      # Perform different modifications based on test_input
#     if test_input.lower() == 'flip':
#         # Flip the colors
#         modified_image = cv2.flip(image, 1)
#     elif test_input.lower() == 'rotate':
#         # Rotate the image
#         modified_image = cv2.rotate(image, cv2.ROTATE_90_CLOCKWISE)
#     elif test_input.lower() == 'change_color':
#         # Change the color of the fish to another color (e.g., red)
#         modified_image = change_fish_color(image, (0, 0, 255))  # (B, G, R)
#     else:
#         # If no valid test_input, return the original image
#         modified_image = image

#     return modified_image




# # Upload image
# image = upload_image()
# if image is not None:
#     test_input = input("Enter a test input ('flip', 'rotate', or 'change_color'): ")
#     modify_image(image, test_input)

"""# ***Last Model***"""

# !pip install diffusers --upgrade
# !pip install transformers accelerate safetensors

# import torch
# from diffusers import (
#     StableDiffusionXLPipeline,
#     EulerAncestralDiscreteScheduler,
#     AutoencoderKL
# )

# # Load VAE component
# vae = AutoencoderKL.from_pretrained(
#     "madebyollin/sdxl-vae-fp16-fix",
#     torch_dtype=torch.float16
# )

# # Configure the pipeline
# pipe = StableDiffusionXLPipeline.from_pretrained(
#     "cagliostrolab/animagine-xl-3.0",
#     vae=vae,
#     torch_dtype=torch.float16,
#     use_safetensors=True,
#     variant="fp16"
# )
# pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)
# pipe.to('cuda')

# # Define prompts and generate image
# prompt = "1girl, arima kana, oshi no ko, solo, upper body, v, smile, looking at viewer, outdoors, night"
# negative_prompt = "nsfw, lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry, artist name"

# image = pipe(
#     prompt,
#     negative_prompt=negative_prompt,
#     width=832,
#     height=1216,
#     guidance_scale=7,
#     num_inference_steps=28
# ).images[0]

"""# ***AutoPipelineForText2Image***"""

# from diffusers import AutoPipelineForText2Image
# import torch

# pipeline = AutoPipelineForText2Image.from_pretrained('dataautogpt3/OpenDalleV1.1', torch_dtype=torch.float16).to('cuda')
# image = pipeline('black fluffy gorgeous dangerous cat animal creature, large orange eyes, big fluffy ears, piercing gaze, full moon, dark ambiance, best quality, extremely detailed').images[0]

# import tensorflow as tf
# import numpy as np
# import matplotlib.pyplot as plt



# image_shape =(255,255,255)
# text_shape = (200,)
# # Define the generator model
# def build_generator(image_shape, text_shape):
#     # ... (as previously defined)
#     # Example usage
#     image_shape = (64, 64, 3)  # Adjust based on your images
#     text_shape = (100,)  # Adjust based on your text representation


# # Build the generator model
# generator = build_generator(image_shape, text_shape)

# # Load pre-trained weights (replace 'path_to_your_weights' with the actual path)
# generator.load_weights('path_to_your_weights')

# # Function to generate an image based on input text
# def generate_image_from_text(text):
#     # Generate random noise for the image input
#     random_image = np.random.normal(size=(1,) + image_shape)

#     # Convert the text to the expected shape
#     text_input = np.random.normal(size=(1,) + text_shape)

#     # Generate an image based on the random image and input text
#     generated_image = generator.predict([random_image, text_input])

#     return generated_image[0]

# # Example usage: Generate an image based on text
# generated_image = generate_image_from_text("a colorful bird on a branch")

# # Display the generated image
# plt.imshow(generated_image)
# plt.axis('off')
# plt.show()

# import tensorflow as tf
# import numpy as np
# import matplotlib.pyplot as plt

# image_shape = (255, 255, 255)
# text_shape = (200,)

# # Define the generator model
# def build_generator(image_shape, text_shape):
#     # Example architecture (modify based on your actual model)
#     image_input = tf.keras.layers.Input(shape=image_shape)
#     text_input = tf.keras.layers.Input(shape=text_shape)

#     # Concatenate image and text inputs
#     concatenated_input = tf.keras.layers.Concatenate()([image_input, text_input])

#     # Add your generator architecture here
#     # ...

#     # Example output layer (modify based on your actual model)
#     output_layer = tf.keras.layers.Conv2D(3, (3, 3), activation='sigmoid', padding='same')(concatenated_input)

#     model = tf.keras.models.Model(inputs=[image_input, text_input], outputs=output_layer)
#     return model

# # Build the generator model
# generator = build_generator(image_shape, text_shape)

# # Load pre-trained weights (replace 'path_to_your_weights' with the actual path)
# weights_path = 'path_to_your_weights'  # Replace with the actual path
# generator.load_weights(weights_path)

# # Function to generate an image based on input text
# def generate_image_from_text(text):
#     # Generate random noise for the image input
#     random_image = np.random.normal(size=(1,) + image_shape)

#     # Convert the text to the expected shape
#     text_input = np.random.normal(size=(1,) + text_shape)

#     # Generate an image based on the random image and input text
#     generated_image = generator.predict([random_image, text_input])

#     return generated_image[0]

# # Example usage: Generate an image based on text
# generated_image = generate_image_from_text("a colorful bird on a branch")

# # Display the generated image
# plt.imshow(generated_image)
# plt.axis('off')
# plt.show()

# pip install git+https://github.com/huggingface/diffusers
!pip install transformers accelerate safetensors

# from diffusers import StableDiffusionXLPipeline
# import torch
# pipe = StableDiffusionXLPipeline.from_pretrained("segmind/SSD-1B", torch_dtype=torch.float16, use_safetensors=True, variant="fp16")
# pipe.to("cuda")
# # if using torch < 2.0
# # pipe.enable_xformers_memory_efficient_attention()
# prompt = "An astronaut riding a green horse" # Your prompt here
# neg_prompt = "ugly, blurry, poor quality" # Negative prompt here
# image = pipe(prompt=prompt, negative_prompt=neg_prompt).images[0]

# from diffusers import StableDiffusionXLPipeline
# import torch

# # Load the pre-trained model
# pipe = StableDiffusionXLPipeline.from_pretrained(
#     "segmind/SSD-1B",
#     torch_dtype=torch.float16,
#     use_safetensors=True,
#     variant="fp16"
# )
# pipe.to("cuda")

# # Define prompts
# prompt = "An astronaut riding a green horse"
# neg_prompt = "ugly, blurry, poor quality"

# # Generate an image
# generated_image = pipe(prompt=prompt, negative_prompt=neg_prompt).images[0]

# # Display or save the generated image
# # (You may need to convert the generated tensor to a numpy array and adjust the display based on the library you're using)
# generated_image_np = generated_image.cpu().numpy().transpose(1, 2, 0)
# plt.imshow(generated_image_np)
# plt.axis('off')
# plt.show()

# from diffusers import StableDiffusionXLPipeline
# import torch
# import matplotlib.pyplot as plt

# # Load the pre-trained model
# pipe = StableDiffusionXLPipeline.from_pretrained(
#     "segmind/SSD-1B",
#     torch_dtype=torch.float16,
#     use_safetensors=True,
#     variant="fp16"
# )
# pipe.to("cuda")

# # Define prompts
# prompt = "An astronaut riding a green horse"
# neg_prompt = "ugly, blurry, poor quality"

# # Generate an image on the GPU
# generated_image = pipe(prompt=prompt, negative_prompt=neg_prompt).images[0]

# # Transfer the generated image to the CPU for visualization
# generated_image_cpu = generated_image.cpu().numpy().transpose(1, 2, 0)

# # Display or save the generated image
# plt.imshow(generated_image_cpu)
# plt.axis('off')
# plt.show()

"""# ***LCMScheduler(test)***"""

# import torch
# from diffusers import AutoPipelineForImage2Image, LCMScheduler
# from diffusers.utils import make_image_grid, load_image

# pipe = AutoPipelineForImage2Image.from_pretrained(
#     "Lykon/dreamshaper-7",
#     torch_dtype=torch.float16,
#     variant="fp16",
# ).to("cuda")

# # set scheduler
# pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)

# # load LCM-LoRA
# pipe.load_lora_weights("latent-consistency/lcm-lora-sdv1-5")
# pipe.fuse_lora()

# # prepare image
# url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png"
# init_image = load_image(url)
# prompt = "Astronauts in a jungle, cold color palette, muted colors, detailed, 8k"

# # pass prompt and image to pipeline
# generator = torch.manual_seed(0)
# image = pipe(
#     prompt,
#     image=init_image,
#     num_inference_steps=4,
#     guidance_scale=1,
#     strength=0.6,
#     generator=generator
# ).images[0]
# make_image_grid([init_image, image], rows=1, cols=2)

"""# ***My Image on Latent Consistency Model (LCM)***"""

#how to  install diffusers
# pip install diffusers

from diffusers import AutoPipelineForImage2Image, LCMScheduler
from diffusers.utils import make_image_grid, load_image
import torch
import IPython.display as display
from PIL import Image

# Initialize the pipeline
pipe = AutoPipelineForImage2Image.from_pretrained(
    "Lykon/dreamshaper-7",
    torch_dtype=torch.float16,
    variant="fp16",
).to("cuda")

# Set scheduler
pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)

# Load LCM-LoRA
pipe.load_lora_weights("latent-consistency/lcm-lora-sdv1-5")
pipe.fuse_lora()

# Function to preprocess and generate image
def generate_image_from_uploaded_file(image_path, prompt):
    # Read and preprocess the uploaded image
    uploaded_image = load_image(image_path)
    # Modify the prompt based on the uploaded image
    modified_prompt = f"{prompt}, uploaded image features"


    # Generate an image with the model

    #generator = torch.manual_seed(0)
    generated_image = pipe(
        modified_prompt,
        image=uploaded_image,
        num_inference_steps=4,
        guidance_scale=1,
        strength=0.6,
        #generator=generator
    ).images[0]
    return uploaded_image, generated_image

# Allow the user to upload an image
uploaded_image_path = "/content/street.jpg"  # Provide a default path if needed
uploaded_image = Image.open(uploaded_image_path)

# Display the uploaded image
display.display(uploaded_image)

# Prompt for user input
prompt = input("Enter a prompt for image generation: ")

# Generate and display the image
uploaded_image, generated_image = generate_image_from_uploaded_file(uploaded_image_path, prompt)
make_image_grid([uploaded_image, generated_image], rows=1, cols=2)



# generator = torch.manual_seed(0)
# image = pipe(prompt, num_inference_steps=20, guidance_scale=3, negative_prompt=negative_prompt, generator=generator).images[0]
# image

# cold color palette, muted colors, detailed, 8k

